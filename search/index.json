[{"content":"Polo: an open-source graphical user interface for crystallization screening was just published in the Journal of Applied Crystallography. It is my first academic paper as well as my first first-author paper.\nPlease check out the article at this link!\nBelow are a few of the figures from the article, click to be taken to the full descriptions.\n  ","date":"2021-02-20","permalink":"https://ethanholleman.com/posts/polo_published/","tags":["programming","blogs","Python"],"title":"Polo paper published in the Journal of Applied Crystallography!"},{"content":"Recently I was asked by a friend if they knew about any databases that classified cannabis strains by symptoms people tend to use them to relieve. I didn\u0026rsquo;t know of the existence of any but had heard about leafly.com which catalogues user reviews of various cannabis strains and compiles data on their characteristics.\nI thought this could be a good place for them to start and so I started looking into what it would take to make a webscrapper to pull down all the data leafly has complied on hundreds on cannabis strains.\nIt turns out it didn\u0026rsquo;t take that much.\nFirst, you can browse strains by going to the strains url at https://www.leafly.com/strains. Conveniently, you can iterate through pages by just adding ?page=n where n is whatever page you want.\nI started looking through the html of these pages originally to find a way to pull out urls that would lead to each individual strain\u0026rsquo;s page but leafly (conveniently for me) provides basically all the information it has on the strains listed on a specific page in a nicely formatted json string in a script at the bottom of the page.\nI was using the BeautifulSoup package and so after getting the page content with requests all it took was\njson_script = str(soup.find_all('script', id='__NEXT_DATA__')) clean_json = re.sub('\u0026lt;.*?\u0026gt;', '', json_script) json.loads(clean_json).pop()  to extract, clean and load the json into a dictionary. I did this for all pages and collected 167 json files.\nI then threw together a quick python script to pull out basic attributes about each strain from the json file collection including metrics on the terpene content of each strain and the feelings strains tend to create for users, collectively called \u0026ldquo;effect measurements\u0026rdquo;. These were both expressed as floats but I am not sure what the units could be for either if there are any.\nI wanted to see if I could use classify the phenotype of each strain (sativa, indicia or hybrid) based on the various terpene and / or effect measurements in a way that was at least better than a random guess.\nI choose to go with a random forest model using the randomForest library in R since there are three possible classifications. The variable importance plots generated from the varImpPlot function after training the models on their respective training sets is below.\nI then tested each model\u0026rsquo;s predictive ability by using the models to predict on the validation set and creating a confusion matrix for each model.\nObserved (validation data) are the rows and predictions are columns.\nTerpene confusion matrix\n    Hybrid Indica Sativa     Hybrid 696 32 3   Indica 218 11 0   Sativa 142 0 3    Using terpenes as the explanatory variables was worse then guessing and basically just thought everything was a hybrid.\nEffect measurement confusion matrix\n    Hybrid Indica Sativa     Hybrid 826 84 40   Indica 172 210 1   Sativa 161 1 92    Neither model did great, and there seems to be a preference for hybrids. This may be because hybrids were the most prevalent phenotype in the leafly database. Either way, fun little exercise that traversed web-scrapping, Python and R.\nIf you would like to play around with the processed data (csv) you can download it at this link.\n","date":"2021-02-16","permalink":"https://ethanholleman.com/posts/leafly_data/","tags":["programming","blogs","Python","R"],"title":"Scraping leafly.com cannabis strain data"},{"content":"The way academic citations are measured currently is pretty standardized. Authors of article A accrue a citation whenever their article is directly cited in article B. But there is likely a large amount of work that was cited by article A but not by article B. The authors of this work which indirectly contributed to article B by contributing to article A (which B cites) will not see a citation.\nWhat if instead citing one article triggered a recursive call all the way down the network formed by articles and their citations? Would this end up eventually citing almost all articles in a field? This is basically the six degrees of separation question but the academic articles.\nI was wondering about this and so experimented on a small scale using the PMC API and a little bit of Python. Conveniently, PMC articles are indented by a unique ID, and you can use the PMC API to get the IDs of all the articles a specific article cites. With this basic functionality we can build up citation networks that branch out an arbitrary distance from one specific \u0026ldquo;root article\u0026rdquo;. This can give an idea of an articles connectedness to other academic literature and what it would look like if citing one article triggered this recursive citation cascade.\nPractically, this is limited because not all articles an article cites will be in PMC and therefore the network will be incomplete, but it is good enough for a fun experiment.\nI tested this out on this article and created the graph below by only traversing three \u0026ldquo;layers\u0026rdquo; deep into the network.\nThat would be a lot of extra citations!\nIf you would like to try this on your favorite PMC article you can download the code from the GitHub page at this link.\n","date":"2021-02-13","permalink":"https://ethanholleman.com/posts/recursive_citations/","tags":["R","blogs","Python"],"title":"What would recursive academic citations look like?"},{"content":"Background I recently saw the an article titled Non-invasive early detection of cancer four years before conventional diagnosis using a blood test . This is obviously an impressive claim and so I skimmed through. The methods of the test are very interesting; the authors measured methylation of DNA in blood plasma and then used logistic regression to classify reads as cancerous or not cancerous in origin based on DNA-methylation patterns extracted from the cancer genome atlas.\nThe paper claims to detect 88% of 5 common cancer types in post-diagnosis patients with a specificity of 96%. Another way of saying the same thing is that their test has a false-negative rate of 12% and a false-positive rate of 4%.\nWhat would it actually mean if you where diagnosed with this test? This is the main question I was thinking about. If you were given this test and it came back positive, what could you conclude? One way to answer this question is determine the Positive Predictive Value (PPV) of the test. PPV effectively asks the question \u0026ldquo;To what extend does a positive test result predict the presence of disease?\u0026rdquo;\n$$ PPV = \\Pr( Cancer \\mid + ) $$\nDetermining an (very) approximate value for the PPV of this test can give us a better idea of how useful this test might actually be as a tool for early cancer diagnosis. To do so, lets consider a concrete example.\nConsider a case where 10,000 people are tested with the new PanSeer test. The National Cancer Institute estimates the incidence rate of cancer in the U.S to be 442.5 per 100,000 men and women. Using this as a rough measure, we might expect ~44 individuals in our 10,000 patient sample to actually have cancer.\nWe can visualize these groups as two blocks, red being the true positives (those with cancer) and green being the true negatives (no cancer).\nFirst we know the test to have a 12% false negative rate, meaning 12%, or ~5 of the 44 patients with cancer will test negative. We also know the test to have a false positive rate of 4%, so we can expect ~398 patients who do not have cancer to test positive.\nI have showed this in the image below by labeling the false negatives in dark red and the false positives in yellow.\nNow we can add take the block representing the true positives and divide it by the sum of the false-positives and false-negatives to get the PPE.\nThis gives PPE = 0.096 or 9.6%. This means if you were to receive a positive test result from the PanSeer blood test you would have (very roughly) a ~9.6% change of actually having cancer. It should be noted though that for simplicity this is treating the test as predicting the occurrence of cancer at most one year into the future. However even this approximation paints a different picture of the test than just stating the false positive and negative rates.\nConclusions The results of this new blood based test are extremely impressive, and hopefully the test becomes even more accurate in the future. Since this article focused on the new methodology the PanSeer test used and their impressive preliminary results I think it was appropriate to focus on measures such as sensitivity and specificity. The purpose of this except was to highlight how the prevalence of a disease has a huge impact on how we interpret the results out in the wild. It is also important to note that PPV can be made arbitrary low by decreasing the prevalence of a disease and that there is a sort of paradox where even an extremely accurate (in terms of sensitivity and specificity) test can have low PPV if the prevalence of the disease is also very low.\nI think the broader take away is that medical testing is inheritably, mathematically difficult endeavour and disease prevalence should be considered by patients and doctors when reviewing the results of any medical test.\n","date":"2021-02-11","permalink":"https://ethanholleman.com/posts/panseer_test/","tags":["medical testing","blogs","cancer"],"title":"PanSeer cancer blood test: Thinking about interpreting medical test accuracy"},{"content":"NBI Background The National Bridge Inventory (NBI) is a program of the Federal Highway Administration which is an agency within the U.S Department of Transportation. The NBI makes available records and statistics about all the bridges in the United States which includes information about bridge location, integrity, inspection history and usage.\nPotential encoding discrepancy As a side project I have been working on creating a more exhaustive Python package for parsing NBI data. This is mainly focused on decoding the numerical representations present in data files to their semantic meanings specified in the NBI documentation.\nI ran into errors when trying to decode the state code fields, which based on the available documentation uses the coding table below.\nThe documentation states the state code will contain three digits, but a quick check of the csv file I was using revealed most state codes had at most 2 digits.\nI wanted to see if this might be true in more files hosted by NBI and so I quickly threw together a Python script to test if this two digit encoding was present in other csv files.\nThe script just looks to see if what is stored in the state code field of a csv file matches any code in the documentation. The results from three files are below.\nDelim file tests ================ 1995.txt: 0 matches and 680662 mismatches 2000.txt: 0 matches and 691060 mismatches 2019HwyBridgesDelimitedAllStates.txt: 0 matches and 617085 mismatches  No state codes in these three files matched the codes from the documentation. However NBI also provides undelimited files, which I believe was the original format as the documentation implies this encoding. Since there is no delimiter, state code is defined as the first three digits in a line.\nUndelim file tests ================== WI19.txt: 14249 matches and 0 mismatches WY19.txt: 0 matches and 3114 mismatches MT19.txt: 5278 matches and 0 mismatches NC19.txt: 18407 matches and 0 mismatches WV19.txt: 7291 matches and 0 mismatches ND19.txt: 4329 matches and 0 mismatches al95.txt: 16576 matches and 0 mismatches ak95.txt: 1451 matches and 0 mismatches  Except for WY19, state codes match.\nThere is another later (but undated) piece of documentation titled Specification for the National Bridge Inventory Bridge Elements which includes a different encoding for state codes which is shown below.\nThese encodings seem to match better if you do not consider leading zeros, but they will not work for undelimited files.\nBased on these antidotal results, it looks like there could have been an undocumented change in encoding schemes when files where made available in csv format, or a bug in the code used to do the conversion.\nWho cares? Is this a problem? This is in all likelihood not a big deal, but I am a nerd and spent an hour or two looking into it. This is likely just a case of a lack of documentation of small change in encoding practices. However, this data is used by researchers and regulators to understand the state of America\u0026rsquo;s infrastructure and therefore its representation should be as accurate and true to reality as possible. So at the very least I think it iw worth thinking about.\nUpdate I got a very timely response from Samantha Lubkin at the FHWA relating to the state encodings.\nThe State Code as defined in the NBI Coding Guide includes a 3rd digit that is currently obsolete. That digit is omitted from the delimited file, and can be ignored in the nondelimited file. Aside from that 3rd digit, the codes are identical between the SNBIBE and the NBI Coding Guide. (And both are based on FIPS.) Whether this kind of logic exists for other fields, I can’t say. But yes, the NBI Coding Guide is the appropriate reference for both delimited and nondelimited NBI files.  Looks like it was a one-ff thing with just state codes. Thanks again Samantha!\n","date":"2021-02-09","permalink":"https://ethanholleman.com/posts/nbi_encoding/","tags":["programming","blogs","Python","National Bridge Inventory"],"title":"Potential NBI encoding error"},{"content":"This recipe (aside from the electronics) is derived from Joshua Weissman\u0026rsquo;s video, How to Make Real Tonkotsu Ramen.\nIngredients Below are all materials you will need to prepare the soup.\nBroth / soup    Ingredient Quantity Units     Pig trotters 3 lbs   Green onion 1 bunch   Yellow onion 1    Shallot 2    Knob ginger 2 inches   Ramen noodles 1 package    Chashu (Braised pork belly)    Ingredient Quantity Units     Pork belly 2 lbs   Soy sauce 1/2 cup   Mirin 3/4 cup   Sake 1 cup   Ginger knob 2 inches   Green onion 1 bunch   Glove garlic 5 cloves    Tare    Ingredient Quantity Units     Bonito flakes 1/2 cup   Kombu 3 pieces   Soy sauce 3/4 cup   Dried shiitake mushrooms 1/4 cup   Chashu braising liquid 1/4 cup    Electronics (Optional)    Ingredient Quantity     Raspberry Pi 1   DS18B20 Temperature Sensor Module Kit 1    Protocol Prepare electronics So I was planning on monitoring the soup using the DS18B20 temperature sensor but it did not arrive in time so unfortunately I don\u0026rsquo;t have data from that. I also thought it might be cool to monitor if the water level drops below a certain level by taking advantage of the conductivity of the soup by placing two separated wires into the soup at the minimum desired level. If a current can be measured between them (broth is acting as the switch) you don\u0026rsquo;t need to replace the water. I had a difficult time positioning the wires consistently given the boiling liquid and required occasional mixing.\nI believe this is is very possible but will require some more specific hardware and / or modification to the pot itself. Anyway, if full control over your soup is what you desire you can download the code I wrote in anticipation of doing this at this link.\nOnce everything is set up, it should text / email you if soup temperature or broth level gets too low.\nPrepare broth  Add 3lbs trotters to large pot. Cover with 2 inches of water and bring to a rolling boil for 10 minutes. Remove any residue that floats to the top with a spoon or mesh strainer. Empty 3lbs trotters and water into a strainer and rinse to clean. Place trotters back into the large pot and cover with 3 inches of water. Cut 1 bunch green onion into fourths and add to large pot. Cut 2 shallots into fourths, leaving skins on, and add to pot. Cut 1 yellow onion into fourths and add to the pot. Peel 2 2 inch knobs ginger, slice and add to pot. Peel 5 cloves garlic and add to pot. Bring water in large part to a rolling boil. Minimize temperature while still maintaining a low boil. This will be maintained for 12 hours. Stir pot around once an hour. Additionally, add water to restore original level whenever half of the water had been lost.  Prepare chashu  Cut 1 bunch green onion into 2 inch sections. Peel 2 inch knob ginger and slice. Add 1 cup sake, the green onion, the ginger, 3/4 cup mirin, 1/2 cup + 2 Tablespoons soy sauce, four cloves of whole garlic, and 1/3 cup water to a large oven-safe pot. Roll the pork belly length wise and tie with kitchen twine into three separate segments. Place into the pot with other ingredients. Bring chashu to a boil, then low boil and then cover with a lid and place into a 300 degree oven for 3-3.5 hours. Baste every 30 mins.  Prepare the tare  Add three 2 inch peices of kombu to a small pot. Add 3/4 cup water to the pot. Boil the water and then let kombu seep for 10 minutes. Add 1/2 cup bonito flakes to pot and let steep for 5 minutes. After the 5 minutes poor solution through a fine mesh sieve into a medium bowl. Add 3/4 cup soy sauce, 1/4 cup + 2 tablespoons mirin, 1/4 cup chashu braising liquid and 1/4 cup rehydrating liquid used for mushrooms to medium bowl. Stir and lightly salt.  Prepare toppings This should be done within a half-hour of time to serve.\n Finely chop the green part of a green onion. Dehydrate shiitake mushrooms according to package directions. Prepare ice water bath. Bring small pot of water to boil. Add eggs and reduce heat to gentile bowl. Boil eggs for 6-7 minutes then place into ice bath.  Assembling the soup  Boil ramen noodles according to package directions. Slice chashu into into 1/2 inch slices. If cold reheat in pan. Add tare to bowl to taste. Strain broth and add to bowl. Add soft boiled eggs, chashu, mushrooms and sliced green onion to broth.  Done!\nEnjoy Here are a few pictures of my results.\nChashu just out of the oven\nCloseup glam shot of that molten egg and soup\nAbusing portrait mode\n","date":"2021-01-14","permalink":"https://ethanholleman.com/posts/tonkatsu/","tags":["cooking","blogs"],"title":"Tonkotsu Recipe"},{"content":"The past couple days I have been running some ligand docking simulations as part of my current rotation with the Cortopasssi lab using Rosetta. One of these docking simulations involved fitting a small portion of the insulin receptor (IR) the lab is interested in, into a known binding region of the Shc1 protein.\nAny Rosetta docking simulation will require hundreds of repetitions, which generate a significant number of pdb files which show the final conformation of the protein and ligand at the end of a given simulation.\nWhile reading about the best way to aggregate and do analyise on these results I spent a bit of time looking for ways to visualize everything Rosetta spits out.\nThis was partly to sanity check the results quickly and also because 3D protein structures and plots just tend to look cool.\nIndividual images using PyMOL The initial simulation I ran produced 200 pdb files. One approach would be to create images of the ligand-protein interface for each of these pdb files.\nTo do this for 200 images I created a very hacky Python script that collects all the pdb files in a directory then creates a temporary PyMOL script which takes a nice picture of the ligand-protein interface.\nThis Python script is basically just a for loop but below is the PyMOL script that I used. All the {} are filled in using the format function in python with the correct filepaths for a specific pdb file.\nload {} # load this pdb file hide everything set cartoon_fancy_helices = 1 set cartoon_highlight_color = grey70 bg_colour white set antialias = 1 set ortho = 1 set sphere_mode, 5 select ligand, hetatm select pocket, ligand around 4 select pocket, byres pocket distance contacts, ligand, pocket, 4, 2 color 4, contacts preset.ligand_cartoon('all') cmd.show('cartoon', 'all') cmd.delete('docked_pol_conts') cmd.show('cartoon', 'all') cmd.hide('lines') cmd.bg_color('black') cmd.set('cartoon_fancy_helices', 1) cmd.show('sticks', 'pocket \u0026amp; (!(n;c,o,h|(n. n\u0026amp;!r. pro)))') cmd.show('sticks', 'ligand') cmd.hide('lines', 'hydro') cmd.hide('sticks', 'hydro') cmd.center('ligand') cmd.center('ligand') ray 1000,1500 png {} # save the image to this file quit  This produces image like the one below.\nI then used the pillow library along with the score.sc file produced by Rosetta which has metrics on the docking quality to create a (huge) gif of all these images.\nFor some reason I was having an issue where if I changed the font of the text to anything other than the default font, when converted to a gif the text would not render. If anyone has had a similar problem please reach out.\nVisualizing the ensemble using plotly and R The second approach I used was to extract the coordinates of the protein and all ligand conformations from the pdb files and plot them in three dimensions using plotly. This produces a \u0026ldquo;cloud\u0026rdquo; of ligand conformations and is a more easily accessible sanity check to make sure Rosetta was docking in generally reasonable locations based on the input parameters.\nThe atoms are labeled with their identity (either protein or ligand) and their coordinates. They are colored by the Rosetta total_score parameter for the complex. Therefore if the complex scored -120, all atoms of the ligand in that complex will have the color corresponding to the -120 value.\nYou can pan, zoom, and rotate the plot using the controls in the upper right. The top 100 ligand conformations by total score are shown below.\n And the top 100 ligand conformations by interface delta X (difference in stability between bound and unbound states) are below this text.\n ","date":"2021-01-12","permalink":"https://ethanholleman.com/posts/ligand_plotting/","tags":["programming","blogs","Python","R","PyMOL"],"title":"Visualizing ligand docking results with PyMOL scripting and R"},{"content":"I spent most of the day today learning about Javascript and CSS by building a (very ameutur) website that you can use to test your Poker pot odds calculation skills.\nDetermining pot odds is useful as when compared to the probability of winning a hand the call\u0026rsquo;s expected value can be approximated.\nYou can visit the website at potoddsquiz.com view the the code at the GitHub page or use the website embedded directly below.\n This is the first Javascript project I have built from scratch and hosted somewhere and was a great way to start learning more about the very basics of web development and the Javascript language.\n","date":"2021-01-02","permalink":"https://ethanholleman.com/posts/potodds/","tags":["programming","blogs","poker"],"title":"PotOddsQuiz.com: A 1 day intro to Javascript and CSS"},{"content":"Are you using the UC Davis FARM for molecular modeling and need to figure out how to setup GROMACS? Well hello extremely small subset of the population! This is the guide for you.\nNote, this is only for a basic installation. For maximum performance refer to the GROMACS guide linked above.\nGetting started We will be working off the installation instructions on the GROMACS website but will modify a few steps to deal with the quirks of the FARM at the time of writing and the fact you will not have sudo privileges.\nIf you want to cut to the chase, you can run this script, which will run all the code in this guide in one go. Everything will be downloaded / complied in the directory you run the script in. You can use the command below to download the script, give it permission to run, and then run it.\nwget http://ethanholleman.github.io/posts/code/farm_install_gromacs.sh; chmod 777 farm_install_gromacs.sh; ./farm_install_gromacs.sh  With that in mind the first thing to do is select a directory where you will download everything, cd into it and get going.\nDownload and install a recent cmake version At the time of writing, the FARM is running cmake 3.10.2 while GROMACS requires at least 3.13.0 to build. I will be downloading 3.19.2 as it is the most recent at the time of writing. If you are downloading a different version you will have to modify some of the commands to reflect that (this will be true for all downloaded programs).\nDownload and run the install with the commands below\nwget https://github.com/Kitware/CMake/releases/download/v3.19.2/cmake-3.19.2-Linux-x86_64.sh chmod 777 cmake-3.19.2-Linux-x86_64.sh echo \u0026quot;Running cmake installer\u0026quot; ./cmake-3.19.2-Linux-x86_64.sh  Download and compile GROMACS Download your preferred version of GROMACS from the docs, I will be using 2021-rc1 for this guide.\nwget http://ftp.gromacs.org/pub/gromacs/gromacs-2021-rc1.tar.gz tar xfz gromacs-2021-rc1.tar.gz  Now enter into the newly downloaded GROMACS directory and create a build directory.\ncd gromacs-2021-rc1 mkdir build cd build  From the build directory run the newly downloaded cmake version by determinging the path to the cmake exe. It will be located in folder produced by the cmake installer. For me, the full path was /home/ethollem/software/cmake-3.19.2-Linux-x86_64/cmake-3.19.2-Linux-x86_64/bin/cmake.\nThen run cmake with your version subsituted into [cmake] below.\n[cmake] .. -DGMX_BUILD_OWN_FFTW=ON -DREGRESSIONTEST_DOWNLOAD=ON  Then make the program (this could take a while).\nmake make check  The GROMACS guide recommends the following command\nsudo make install  but since we do not have sudo privileges to run the program we can add the exe path to a variable in ~/.profile to use GROMACS from anywhere. From the build directory the GROMACS exe path will be [Your absolute path to build dir]/bin/gmx. For me this looks like home/ethollem/software/gromacs-2021-rc1/build/bin/gmx.\nOpen / create ~/.profile using your preferred text editor and add the line\nexport gmx=\u0026quot;/home/ethollem/software/gromacs-2021-rc1/build/bin/gmx\u0026quot;  Then log off and log back in again. Test everything is working correctly by running\n$gmx --help  You should be greeted wih the GROMACS help page. If you are you are now good to go.\nThis is not ideal as you will have to run GROMACS using $gmx instead of gmx but since we cannot create symbolic links to files in /usr/local/bin it is the solution I am working with for now.\n","date":"2020-12-30","permalink":"https://ethanholleman.com/posts/gromacs_install/","tags":["programming","blogs","guides"],"title":"Installing GROMACS on the UC Davis FARM cluster: or install GROMACS without sudo privileges."},{"content":"Day trip to Yosemite National Park.\nHike up to mirror lake   Campground near Curry Village\n    Good advice.\n    Ethan with log on the trail.\n    Erica with boulder.\n    Moss 1.\n    Moss 2.\n    Moss 3 (lots of interesting mosses).\n  At mirror lake   Mirror Lake visitor info placard.\n    Mirror lake facing Northwest towards Mt. Watkins.\n    Half Dome.\n    Ethan and Erica in front of Half Dome.\n    Interesting log, ground in the log\u0026rsquo;s shadow remained frozen.\n    Panoramic view facing Northeast with (from left to right) Mt. Watkins, Ahwiyah Point and Half Dome in view.\n  ","date":"2020-12-29","permalink":"https://ethanholleman.com/posts/yosemite/","tags":["day trips","blogs"],"title":"Day trip to Yosemite"},{"content":"Skribbl.io is a great free quarantine / social distanced game where one person attempts to draw a word while everyone else guesses what they are drawing. When setting up the game you can supply your own list of comma separated words doing the game.\nThe problem with doing this manually is that one person playing will know all the words.\nFor an upcoming Zoom party I created a python command line application that takes in subreddit names and a few other parameters and using the Praw library retrieves the most commonly used words from the top comments of posts to a subreddit.\nSince subreddits are generally devoted to a specific topic you can easily create pseudo-themed word banks by pulling comments from a category of topics and selecting subreddits under that banner.\nYou can download the program from the GitHub page.\nUsage Install dependencies The only dependcy you need is Praw. Install it with the command below.\npip install praw  Setup API keys If you want to run the program yourself you will need to get a client id and client_secret to use the Reddit API through Praw. The tutorial below has all the info you need (you only need to watch the setup portion).\n  To be able to post this project on GitHub (relatively) safely I used environmental variables to store the values or my Reddit API credentials. You can do the same or modify the code of collect_reddit_instance function (shown below) in collect.py to use your credentials.\ndef create_reddit_instance(): '''Create a Reddit instance using Praw library. Returns: Reddit: Reddit instance via Praw. Credentials set using environmental variables. ''' return praw.Reddit(client_id=os.environ['PRAW_CLIENT_ID'], client_secret=os.environ['PRAW_SECRET'], user_agent=os.environ['PRAW_USER_AGENT'] )  Set values for PRAW_CLIENT_ID, PRAW_SECRET and PRAW_USER_AGENT or modify the code directly with your credentials.\nRun the program Once you have that set up you are ready to run the program by executing the run.py file. The help menu it will print is below.\npython run.py --help usage: run.py [-h] [-r SUBREDDITS [SUBREDDITS ...]] [-n NUMBER_WORDS] [-mc COMMENT_LIMIT] [-o OUTPUT_DIR] [-f] [-p POST_LIMIT] [-l MIN_WORD_LENGTH] Harvest frequently words from subreddit comments using Praw optional arguments: -h, --help show this help message and exit -r SUBREDDITS [SUBREDDITS ...], --subreddits SUBREDDITS [SUBREDDITS ...] List of subreddits to pull comments from -n NUMBER_WORDS, --number_words NUMBER_WORDS Number of words to return per subreddit. Defaults to 25. -mc COMMENT_LIMIT, --comment_limit COMMENT_LIMIT Max number of comments to harvest per subreddit. Defaults to 10,000 -o OUTPUT_DIR, --output_dir OUTPUT_DIR Output directory to write results to. -f, --include_occurrences Include the number of times each word appears in the final output -p POST_LIMIT, --post_limit POST_LIMIT Max number of posts to harvest from. Defaults to 50. -l MIN_WORD_LENGTH, --min_word_length MIN_WORD_LENGTH Min word length. Defaults to 3 characters.  For example if I wanted to get the 10 most frequently used words from 100 comments from r/DataHoarder I would use the command\npython run.py -r \u0026quot;DataHoarder\u0026quot; -n 10 -mc 100  You can also specify multiple subreddits. The top word for each subreddit will be written to a separate text files.\npython run.py -r \u0026quot;DataHoarder\u0026quot; \u0026quot;Python\u0026quot; \u0026quot;arduino\u0026quot; -n 10 -mc 100  Or use pre-harvested words If you do not want to set up the program on your own computer I have already created lists of 25 most used words with 5 or more characters from top comments of the 100 most popular subreddits.\nYou can download those files from the GitHib page here. Words are listed on a single line separated by command for easier input into\nNOTE\nI have not reviewed all the words in these files and do not endorse any of the content that may be found within, this is the internet after all.\n","date":"2020-12-28","permalink":"https://ethanholleman.com/posts/skribbl.io_words/","tags":["blogs","Python"],"title":"Make custom Skribbl.io word banks using Reddit and Praw"},{"content":"After finding the COG-UK data I was looking around for other interesting COVID-19 datasets to play around with and build my R plotting skills with.\nUser moritz.kraemer posted this article on early case descriptions which included a lot of geo-spacial data that I was interested in takeing a look at.\nThere was a significant number of fields devoted to hospitalization related measurements and so I focused on that subject for the plot below.\nThe dataset includes patients with and without hospitalization records and so first I filtered down to just those with records and those who also had location data. This subset of patients formed subplot A. Clearly this is not an exhaustive dataset and seems to focus on hospitalizations for China, France, and Argentina.\nSubplot B shows hospitalizations by date, which for this dataset, really pick up in April. However, this is not super informative due to ~90 % of the data being localized to three countries.\nFinally, subplot C includes two boxplots. The first shows the difference in days between a patient experiencing symptoms and admission to a hospital. A negative value indicates days before admission. So a patient with a value of -10 would mean the patient experienced symptoms 10 days before being admitted. The second boxplot shows the length of stay once admitted. It should be noted that this dataset lumps deaths and discharges into the same category date_death_or_discharge. The mean stay once admitted was ~10 days.\nThe R code to generate these plots can be viewed here\n","date":"2020-12-24","permalink":"https://ethanholleman.com/posts/covid_data/","tags":["programming","R","blogs"],"title":"Plotting COVID-19 Hospitalization Geo-Spacial Data"},{"content":"The Covid-19 Genomics UK Consortium has been collecting and sequencing thousands of COVID-19 genomes from patients in the UK and around the world.\nAll of their data is publicly available. Here I played around with the phylogenetic tree they have created from global alignments of all the genomes they have sequenced.\nYou can download the tree in Newick format from their data page which also hosts sequences and the alignment files.\nVisualizing the COVID-19 phylogenetic tree by country of origin Genome count by country Note this plot is log scale in the y-axis.\n16 most prevalent UK COVID-19 lineages Density plots showing the number of genomes of the 16 most prevalent lineages detected by COG-UK.\nCov-lineages has a lot of good information on all of these lineages, including B.1.177 which has recently set off alarms as the new \u0026ldquo;mutant UK strain\u0026rdquo;.\nCode used to make the above plots can be viewed here.\n","date":"2020-12-21","permalink":"https://ethanholleman.com/posts/cog_sars/","tags":["programming","R","blogs"],"title":"Plotting COG-UK Data"},{"content":"Apartment Tour Taken right after move in.\nSkys during the wildfires Those are not clouds.\nDavis campus cows Around Davis Hiking in Winters, CA\nFishing at Putah Creek\nThanksgiving Bubbles Lots cat pictures have been taken\nCalifornia changes a man ","date":"2020-12-20","permalink":"https://ethanholleman.com/posts/christmas_card/","tags":["blogs","christmas card"],"title":"2020 Christmas Card Bonus Pictures"},{"content":"Y axis of all plots are unit-less.\nRunning stack sizes Shows each players cumulative stack over all nights played.\n Winnings by date Plot of each players winnings (stack at the end of the round - buy in) over time.\n All games have been played virtually, for info on how we host these see this post.\n","date":"2020-12-18","permalink":"https://ethanholleman.com/posts/running_winnings/","tags":["poker","blogs"],"title":"Poker Nights Running Results"},{"content":"Update I recently found a number of .io type websites devoted specifically to poker (not sure why didn\u0026rsquo;t think to search poker .io earlier) which get around the main issues of playing using playingcards.io.\nI would highly, highly recommend using lipoker.io. It handles betting with shortcuts for bets based on pot or blind sizes, handles turns automatically and does not require a sign up.\nFor Poker, lipoker is supporior in every way to PlayingCards.io due to being designed for this specific game. If for some reason you would still like to use PlayingCards.io continue reading, otherwise stop and use lipoker.io.\nRecently a few of my friends and I wanted to do virtual Poker night. We where not interested in playing for cash and so I started looking around for an online platform to make it happen.\nPretty much everything that comes up with a cursory google search did not satisfy my basic requirements but eventually I came across PlayingCards.io which provided what I think is the best solution for free, causal quarantine poker nights.\nStep 1 is to set up your \u0026ldquo;table\u0026rdquo; with PlayingCards. Use this link to start building a table. Later once you finish setting everything up you can share the link to the table and others will be able to join, see and manipulate the objects on the table.\nYou can use the automation buttons to handle things like dealing, the flop, turn, river shuffling, as well as betting. The room I set up looks like this.\nYou can deal cards to each player using the Deal button and show each round of community cards using the respectively labeled button. Empty sets the pot to 0, Reset sets the pot to 0 and all player stacks to 100 and Reveal shows all players cards.\nThe only thing that is a little clunky is betting and collecting your winnings. The automations allow you to add or subtract a value from a counter (which are used to keep track of each player\u0026rsquo;s stack and the current pot) but not read from the state of another counter. This means you cannot create a button that adds the current value in the pot to a player\u0026rsquo;s stack - this needs to be done by hand.\nAlso anyone can press any button at any time; just something to keep in mind if playing with anyone that you thought of when reading that sentence.\nNote that cards placed into the \u0026ldquo;hand\u0026rdquo; area can only be seen by the player that put them there. This makes keeping your pocket cards secret from everyone else possible.\nIf you don\u0026rsquo;t want to set up your own room from scratch you can download the room I set up and then load from that file.\nNo Poker is actually done over Zoom, it is only used as the video call medium.\nIt definitely look a couple hands to get used to using the interface but once things got going it worked really well. We found it also helps to keep track of your stack either on paper or using small text boxes on the virtual table.\nHere is a gif of me playing out a hand. Normally the other players would take their cards out of the dealt slots and keep them in their \u0026ldquo;hand\u0026rdquo; area where they are not visible to other players until showdown. If a player decides to fold they place their cards back into their dealt slots.\n","date":"2020-12-15","permalink":"https://ethanholleman.com/posts/zoom_poker/","tags":["poker","blogs"],"title":"Casual virtual Poker with Zoom and PlayingCards.io"},{"content":"Turning the jungle into punch cards During the early optimistic days of the summer 2020 quarantine I watched Ken Burn\u0026rsquo;s fantastic 10 part 18-hour series on the Vietnam war. It is by far the most accessible and compressive body of work on the subject. Burn\u0026rsquo;s starts you off pre-WWI so you really get a comprehensive picture of things.\nOne of the aspects of the war that fascinated me the most was the push by the the then Secretary of Defense, Robert McNamara, to quantify as much of the war as was computationally possible. One of the most storage intensive of McNamara\u0026rsquo;s efforts was the Hamlet Evaluation System, which attempted to quantify the degree to which ~12,000 small, rural Vietnamese villages had been pacified. A RAND corporation report found the program was producing 90,000 pages of data every month. More than could have ever been useful.\n  Example of data produced by the Hamlet Evaluation Program showing change in hamlet classifications over time   This was the data produced by just one wartime metrics program. Even if the DoD had the raw 60s era computing power and the army of FORTRAN programers that would have been needed to wrangle it all, the metrics themselves were questionable at best. One of the historians interviewed as apart of Burn\u0026rsquo;s series said something along the lines of \u0026ldquo;When you can\u0026rsquo;t measure whats counts, you make what can count the measure\u0026rdquo;.\nI was interested in actually seeing what that data looked like in its raw form. What where programmers of the era actually looking at and wrangling when some Army big-wig said \u0026ldquo;We need to be producing 90,000 pages of data a month\u0026rdquo;? So I did some googling and dug into a couple documents hosted my the National Archives to try and get a picture of what Vietnam looked like from the perspective of a punch card.\nThe Phung Hoang Mangement Information System The degree of documentation relating to the Hamlet Evaluation System that survives to today is, somewhat unsurprisingly, extremely large. So picking one out to dive into was a less than analytical process.\nA came across the Phung Hoang Mangement Information System, PHMIS, (MACV Document Number DAR R33 CM-01A, March 1972) which was later replaced with the National Police Infrastructure Analysis Subsystem; a database cataloging the personal information of Vietnamese who where suspected of or convicted of aiding communist forces as part of the Hamlet Evaluation Program. This entry was interesting to me because it had both the technical documentation needed to actually make sense of the data and because of its historical context. The PHMIS was used as a catalogue for the operations of the Phoenix program a CIA headed counter-insurgency operation that among other techniques, employed torture and assassination to identify and kill Viet-Cong and Viet-Cong collaborators.\n  Flowchart depicting data schema of the Phung Hoang Mangement Information System, March 1972 report   The Phoenix Program was, deservingly, a locus of controversy within a storm of controversies surrounding US operations in Vietnam, eventually building to a series of congressional hearings in 1971. The data stored in this National Archive entry, in some way, is the bureaucratic reflection of all the individual stories and lives impacted by this corner of red-scare induced mania.\nGetting into the data The PHMIS National Archive entry contains one main data file and some aggregated technical documentation.\n RG330.PHMIS.P6972: The actual binary data file. Refered to as RG330 222.1SPA.pdf: Historical background on the PHMIS 222.1DP.pdf: Technical documentation on format of RG330  At first I was a bit lost as to where to start after thumbing through the technical documentation. At first I naively tried to read RG330 as UTF-8 but then soon remembered unicode was not created until the 1980s.\nThankfully I found the technical specifications summary provided by the National Archives which had this critical information.\nThe number of records, the length of each record and critically the encoding: EBCDIC. At this point I had no idea what EBCDIC encoding was but some quick googling corrected that. Basically, EBCDIC (Extended Binary Coded Decimal Interchange Code) was created by IBM in the late 1950s and used a perplexing (see EBCDIC wikipedia page criticism and humor) eight-bit character encoding for mainframe computers.\n  Example of EBCDIC encoded punch card.    I almost stopped here, but the Python community came through once again and thanks to Thomas Aglassinger you can successfully run the command\npip install ebcdic  and have a number of EBCDIC encodings to work with right in Python. And the secrets of RG330 can be revealed in their UTF-8 glory with the Python snippet below.\nimport ebcdic filepath = './RG330.PHMIS.P6972' data = open(filepath, 'rb').read().decode('cp1141')  Wrangling The next step was to get the decoded mass of information into something that could be output to a csv file and would be much easier to work with. The technical documentation that let us know RG330 was EDCDIC encoded also says that the record length is 319 so my approach was to go with that and pray to the data-wrangling Gods.\nstep = 319 print(data[:step])  gives\n01000005A207000069120H691270122E70125041100002CB 34KKKKK5C12070103001BM########################00000ä 00000ä 00000ä 00000ä 00000ä 00000ä 00000ä 00000ä 0ä ########################################################################  which as a first attempt does not actually look that bad. The National Archives documentation notes that a personally identifying information in the database has been redacted for public use - which is showing up as the # characters. A memo from December 10, 1992 states the following.\nI used this to verify the correctness of my naive parsing approach. If everything lines up, the only characters we should be finding in columns 73-96, 248-271, 272-295 and 296-319 are #s.\nredacted_base_1 = [(73, 96), (248, 271), (272, 295), (296, 319)] redacted_base_0 = [(r[0] - 1, r[1]) for r in redacted_base_1] for redacted_region in redacted_base_0: start, end = redacted_region assert set(data[start:end]).pop() == \u0026quot;#\u0026quot;  Does not produce any assertion errors so lets expand to the whole dataset.\nfor i in range(0, len(data), 319): record = data[i:i+step] for redacted_region in redacted_base_0: start, end = redacted_region assert set(record[start:end]).pop() == \u0026quot;#\u0026quot;  Which also does not produce any assertion errors, so things are looking pretty good at this point. The two items of main concern are the ä characters and the large amount of whitespace. Some more investigation revealed the reason for the whitespace. The document was originally stored in a variable record length format and was converted to a fixed length by the National Archives at some point. Relevant documentation in its original photocopy glory below.\nWhile I am not as certain about the cause of the diacritic a character, I believe it is due to the use of zoned decimal formatting in some of the fields. The documentation notes its use in general, but does not provide specific indices. Since it is not affecting the integrity of the actual record parsing I am ignoring apparent zoned decimal fields for the time being.\nThe technical documentation also defines the range of each field within a record that we can use to make the final output file more explicitly delimitated. A sample of which is shown below.\nSo now it is just a matter of creating a function that will cut up a record based on the fields defined in this table. I first used PyPDF2 to pull out the text from the table I was interested in.\nimport PyPDF2 pdf = open('222.1DP.pdf', 'rb') reader = PyPDF2.PdfFileReader(pdf) pages = ''.join([reader.getPage(p).extractText() for p in [20, 21, 22, 23]]) print(pages[:100])  Which looks like this\nINPUT, OUTPUT, MASTER DEFINITION (Excluding Reports) I. PAGE 1 OF 4 5. DATE PREPARED 9/1/77 2. NAME  Its a jumble but thankfully, there are some patterns that can be exploited to reduce manual work required to get everything correctly formatted. Specifically after the range of each field it is followed by either an A or a N, signifying if that data is numeric or alphanumeric. We can use this pattern in a regex to roughly pull out what we need.\nimport re finder = re.compile(r'(.{1,7}) (\\d+-?\\d*) (N|A)') matches = finder.findall(pages) for m in matches[:10]: print(m[0], m[1])  Which prints\n. PROVe 1-2 SE'O'fO 3-8 6 ATLRG 9 !XX)RP 10 DPROV 11-12 . DDIsr 13-14 2 DVllL 15-16 . IDATX 23-26 BDA'IX 33-36 POORP 37  While this is by no means perfect it is looking a lot better. From here I just cleaned things up manually, eventually creating a text file that looks like this\nPROVC 1 2 SEQNO 3 8 ATLRG 9 DOORP 10 . . . ALTVCI 239 247 MOM 248 271 POP 272 295 ALIAS 296 319  Now we can finally create the function that will create the csv file we are after!\nFirst I created a dictionary that could be used to slice each \u0026ldquo;row\u0026rdquo; of the raw data.\nimport csv table = \u0026quot;parse_table.txt\u0026quot; spacing_dict = { r[0]: [int(i) for i in r[1:]] for r in csv.reader(open(table), delimiter=' ') } # adjust for base 1 to base 0 indexing for field in spacing_dict: spacing_dict[field][0] -= 1 # append next index for single index fields for easier slicing in next step for field_name in spacing_dict: if len(spacing_dict[field_name]) == 1: val = spacing_dict[field_name][0] spacing_dict[field_name].append(val+1)  Then I created a function that would actually do the slicing on each raw data row and return a dictionary with the field names as keys and the data in each field\u0026rsquo;s respective domain as values.\ndef raw_data_to_field_dict(raw_data, spacing_dict): field_dict = {} for field_name in spacing_dict: start, end = spacing_dict[field_name] field_dict[field_name] = raw_data[start:end] return field_dict  Now we can test this out to see if we can write our csv file.\ndef write_csv_from_raw_data(raw_data, spacing_dict, outname='data.csv'): csv_rows = [] for i in range(0, len(data), step): row = data[i:i+step] csv_rows.append(raw_data_to_field_dict(row, spacing_dict)) with open(outname, 'w') as handle: writer = csv.DictWriter(handle, fieldnames=spacing_dict.keys()) writer.writeheader() writer.writerows(csv_rows)  The function executes without incident and writes a csv file that looks like the sample below.\nPROVC\tSEQNO\tATLRG\tDOORP\tDPROV\tDDIST\tDVllL 1\t5\t7\t0\t0 1\t11\t1\t2\t0 1\t12\t1\t2\t0  Much easier to read!\nVisualizing Now that the data is in a more accessible format we can start to take examine what it looks like using R and the ggplot2 package.\nArrests by year Unsurprisingly most arrests took place between 70 and 72. Although there were some extreme early outliers in 1900 and 1903 which are likely data entry errors.\nIndividual statuses Fields 126-131 contain the OPINFO information, which the technical documentation describes as a group containing the following information, again as described by the technical documentation.\n STATUS: The status of an individual. TARGET: The type of target, general of specific. LEVEL: The level of operation. Corps, Division, etc. FORCE: Type of action force which was responsible for the neutralization. DEFAC: Place of detention.  This is a lot of interesting information. These values are all stored as one letter codes and the documentation provides tables for translating them, like the one below which is used to translate the STATUS code.\nFirst we can look at just statuses of all individuals in the database to get as sense for what was happening to the people targeted by Project Phoenix.\nWhile the STATUS field is missing for about a third of individuals in the database, clearly the most common outcomes were \u0026ldquo;captured\u0026rdquo;, \u0026ldquo;killed\u0026rdquo; or \u0026ldquo;rallied\u0026rdquo;. While \u0026ldquo;captured\u0026rdquo; and \u0026ldquo;killed\u0026rdquo; are relatively unambiguous, there is not further explanation of what \u0026ldquo;rallied\u0026rdquo; refers to that I could find.\nThis same data can also be broken down in a few interesting ways. We can plot the same STATUS data but split into into subplots based on the FORCE that was responsible for the (coldly bureaucratically termed) neutralization.\n\\\nUsing this plot, we can see that \u0026ldquo;Regional Forces\u0026rdquo; where the most active group, with \u0026ldquo;ARVN Main Forces\u0026rdquo;, \u0026ldquo;Popular Forces\u0026rdquo;, \u0026ldquo;Provincial Reconnaissance Unit\u0026rdquo; making up much of the remainder.\nThis plot also shows that US Forces, at least according to this database, where not as nearly as directly involved as organizations that can be grouped into the \u0026ldquo;South Vietnamese Allies\u0026rdquo; category.\nLastly, I was interested in what this program looked like over time. Individuals that were captured (opposed to killed outright) usually had a value in their SADATX field: \u0026ldquo;Date of sentence action in YYMM order\u0026rdquo;. I used this as a proxy for a given group\u0026rsquo;s activity over time, granted this would be easily skewed in the case one group was tasked explicitly with capturing while another was tasked with killing. Plotting SADATX vs the number of individuals for all groups listed by the FORCES field produced the plot below.\nThere is still much to be said I only looked at a small part of this dataset, but there is still much more to be gleaned. If you would like to play around with the data yourself you can download the csv file I produced from this link. You can also download all the documentation I referenced from the National Archives entry at this link.\nThank you for reading.\n-eth\n","date":"2020-12-12","permalink":"https://ethanholleman.com/posts/vietnam_data/","tags":["blogs","data wrangling"],"title":"Data and the Vietnam War"},{"content":"Day trip out to Putah Creek in Winters CA.\nA few turkeys we saw before leaving Davis.\nThe spot in the creek we picked out.\nErica practicing casting.\nTesting out the waders.\n","date":"2020-11-28","permalink":"https://ethanholleman.com/posts/putah_fishing/","tags":["fishing","blogs"],"title":"Fishing at Putah Creek"},{"content":"I ran into a few issues trying to figure out how to run batch scripts on the Genome Center cluster. One of the least documented and hardest to figure out was how to successfully load R packages I had installed to my user directory.\nFor example I would start R and install a package I needed and load the library with no problem.\nThe code below would run without a problem.\ninstall.packages(\u0026quot;glmnet\u0026quot;) library(glmnet)  But then calling the running the same script from a batch file fails to import the library.\nI tried multiple possible fixes with no success until I was tipped of by a current lab member about the magic word.\naklog  For some reason putting this right after the SBATCH lines in the script changed something that allowed SLURM to see the R packages in my user directory.\nIt is not currently documented, but when it hopefully is I will link to the page here.\n","date":"2020-11-03","permalink":"https://ethanholleman.com/posts/genome_cluster/","tags":["programming","blogs","guides"],"title":"Using local R packages on UC Davis Genome Center cluster"}]